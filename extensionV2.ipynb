{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Milestone 4 - Linguistic Harbingers of Betrayal Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, input all the libraries we need to get the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data file contains only the selected games, there is no need to consider the selection criteria when retreving the message data. \n",
    "*The condition \"relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time, with no more than five seasons passing between two acts of friendship\" is fullfilled.\n",
    "\n",
    "Here, we read the data from json file and get the number of games, which is 500, including 250 selected games ended up betrayal and 250 selected games ended up lasting friendship between two players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from diplomacy_data.json with reading mode\n",
    "with open(\"diplomacy_data.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only consider dialogs that happen before the last act of friendship, we need to know when does the last act of friendship happen. Later when extracting feature values, extract them from the seasons that is before the last support season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the season where the last support between two players happened\n",
    "# entry refer to one entry in the dataset, one game\n",
    "def last_support(entry):\n",
    "    seasons = entry['seasons']\n",
    "    last_support = None\n",
    "    for season in seasons[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the average value across the seasons for all features of the messages\n",
    "# msgs: messages sent by a player from a season in a game\n",
    "def extract_features(msgs):\n",
    "    n_sents = sum(m['n_sentences'] for m in msgs) * 1.0\n",
    "    if(n_sents==0.0):\n",
    "        print(msgs)\n",
    "    \n",
    "    # Sentiment\n",
    "    # compute positive sentiment score\n",
    "    sent_pos = sum(m['sentiment'].get(\"positive\") for m in msgs) / len(msgs)\n",
    "    # compute neutral sentiment score\n",
    "    sent_neu = sum(m['sentiment'].get(\"neutral\") for m in msgs) / len(msgs)\n",
    "    # compute positive sentiment score\n",
    "    sent_neg = sum(m['sentiment'].get(\"negative\") for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Argumentation and Discourse\n",
    "    # compute discourse complexity through comparison, contingency, expansive and temporal\n",
    "    comparison = sum(len(m['lexicon_words'].get(\"disc_comparison\", []))for m in msgs) / n_sents\n",
    "    contingency = sum(len(m['lexicon_words'].get(\"disc_contingency\", []))for m in msgs) / n_sents\n",
    "    expansive = sum(len(m['lexicon_words'].get(\"disc_expansion\", []))for m in msgs) / n_sents\n",
    "    temporal = sum(len(m['lexicon_words'].get(\"disc_temporal_rest\", []))for m in msgs) / n_sents\n",
    "    # compute the average number of markers refer to future\n",
    "    plan = sum(len(m['lexicon_words'].get(\"disc_temporal_future\", []))for m in msgs) / n_sents\n",
    "    # compute argumentation level through claim and premise\n",
    "    claim = sum(len(m['lexicon_words'].get(\"claim\", []))for m in msgs) / n_sents\n",
    "    premise = sum(len(m['lexicon_words'].get(\"premise\", []))for m in msgs) / n_sents\n",
    "    argu_level = claim+premise\n",
    "    # compute average number of request\n",
    "    n_request = sum(m['n_requests'] for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Politeness\n",
    "    politeness = sum(m['politeness'] for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Subjectivity using allsubj\n",
    "    subj = sum(len(m['lexicon_words'].get(\"allsubj\", []))for m in msgs) / n_sents\n",
    "    \n",
    "    # Talkativeness\n",
    "    # compute average number of words\n",
    "    n_words = sum(m['n_words'] for m in msgs) / len(msgs)\n",
    "    # compute average number of sentences\n",
    "    n_sentences = sum(m['n_sentences'] for m in msgs) / len(msgs)\n",
    "\n",
    "    return np.array([sent_pos, sent_neu, sent_neg,comparison,contingency,expansive,temporal, plan, argu_level, n_request, politeness, subj, n_words, n_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From paper: To ensure that we are studying conversational patterns that occur only when the two individuals in the dyad are ostensibly being friends, we only extract features from the messages exchanged before the last act of friendship.\n",
    "Therefore, the messages we extract should from seasons less than last support season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get average politeness scores from dataset entries \n",
    "# data: the dataset where we extract politeness score from\n",
    "# betrayal: if the games we consider end up betrayal\n",
    "# betrayer: if the person to be analyzed is the (potential) betrayer\n",
    "def process_data(data, betrayal, betrayer):\n",
    "    results = []\n",
    "\n",
    "    # loop in every game in data\n",
    "    for entry in data:\n",
    "        len_seasons = len(entry['seasons'])                   \n",
    "        # a matrix to store features of different seasons of one game\n",
    "        data41game = np.zeros((len_seasons, 14))\n",
    "        cut_ind = 0\n",
    "        # none is used to check if the season contain any valuale msg\n",
    "        none = True\n",
    "                        \n",
    "        if(entry['betrayal'] == betrayal):\n",
    "            for i, season in enumerate(entry['seasons']):\n",
    "                if(season['season'] <= last_support(entry)):\n",
    "                    #if(len(season['messages']['betrayer']) > 0 and len(season['messages']['victim']) > 0):                \n",
    "                    if(betrayer):\n",
    "                        if(len(season['messages']['betrayer']) > 0 and sum(m['n_sentences'] for m in season['messages']['betrayer'])>0):\n",
    "                            none = False\n",
    "                            data41game[i,:] = extract_features(season['messages']['betrayer'])\n",
    "                    else:\n",
    "                        if(len(season['messages']['victim']) > 0):\n",
    "                            none = False\n",
    "                            data41game[i,:] = extract_features(season['messages']['victim'])\n",
    "                else:\n",
    "                    cut_ind = i\n",
    "                    break\n",
    "        \n",
    "        # we consider only games where friendship lasts for at least four season\n",
    "        if(cut_ind>4):\n",
    "            # save only the four seasons before the last support\n",
    "            data41game = data41game[cut_ind-4:cut_ind,:]\n",
    "            # \n",
    "            if(none==False):\n",
    "                data41game = data41game.flat\n",
    "                results.append(data41game)        \n",
    "\n",
    "    # return value is a list of features of the indicated player in the indicated type of games\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the politeness score of the four conditions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the games end up betrayal, get politeness score for each betrayer\n",
    "betray_er = pd.DataFrame(process_data(data, True, True))\n",
    "\n",
    "# for the games end up betrayal, get politeness score for each victim\n",
    "betray_vi = pd.DataFrame(process_data(data, True, False)) \n",
    "\n",
    "# for the games not end up betrayal, get politeness score for each potential betrayer\n",
    "control_er = pd.DataFrame(process_data(data, False, True))\n",
    "\n",
    "# for the games not end up betrayal, get politeness score for each potential victim\n",
    "control_vi = pd.DataFrame(process_data(data, False, False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "betray_er['betray'] = 1\n",
    "control_er['betray'] = 0\n",
    "\n",
    "frames = [betray_er,control_er]\n",
    "Betrayer = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "betray_vi['betray'] = 1\n",
    "control_vi['betray'] = 0\n",
    "\n",
    "frames2 = [betray_vi,control_vi]\n",
    "Victim = pd.concat(frames2,ignore_index=True)\n",
    "\n",
    "frames = [Betrayer, Victim]\n",
    "total= pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the messages sent by a player from a season in a game, we need to calculate the different variable values of the messages.\n",
    "\n",
    "Variables we consider:\n",
    "\n",
    "1. sent_pos: Positive sentiment\n",
    "2. sent_neu: Neutral sentiment\n",
    "3. sent_neg: Negative sentiment\n",
    "4. discourse_comp: Discourse complexity (calculated through comparison, contingency, expansive and temporal)\n",
    "5. plan: Planning level (calculated through future)\n",
    "6. argu_level: Argumentation level (calculated through claim and premise)\n",
    "7. n_request: Number of requests\n",
    "8. politeness: Politeness\n",
    "9. subj: Subjectivity\n",
    "10. n_words: Number of words\n",
    "11. n_sentences: Number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "XI = total.drop(['betray'],axis = 1)\n",
    "yi = total['betray']\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "XI_train, XI_test, yi_train, yi_test = train_test_split(XI, yi, test_size=0.2, random_state=10)\n",
    "\n",
    "# ploy feature\n",
    "XI_train = PolynomialFeatures().fit_transform(XI_train)\n",
    "\n",
    "# standardizer\n",
    "standardizer = StandardScaler().fit(XI_train)\n",
    "\n",
    "XI_train = standardizer.transform(XI_train)\n",
    "\n",
    "XI_test = standardizer.transform(XI_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755223880597015"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "sv =SVC(C=5, kernel='rbf', gamma='auto', coef0=0.0, shrinking=True, probability=False,\n",
    "        decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "\n",
    "predicted = cross_validate(sv, XI_train, yi_train, cv=5)\n",
    "np.mean(predicted['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5294117647058824"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv.fit(XI_train,yi_train)\n",
    "sv.score(XI_test,yi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6488586479367866"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(n_estimators=100,max_depth=15,criterion='gini')\n",
    "predicted = cross_validate(rf, XI_train, yi_train, cv=5)\n",
    "np.mean(predicted['test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176470588235295"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(XI_train,yi_train)\n",
    "rf.score(XI_test,yi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
