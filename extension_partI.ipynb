{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Milestone 4 - Linguistic Harbingers of Betrayal Extension Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider each valuable season as individual. We process the data in the way like in part 4 of the paper *Language Foretelling Betrayal*.  \n",
    "\n",
    "Given the messages sent by a player from a season in a game, we need to calculate the different variable values of the messages. \n",
    "\n",
    "Based on the feature selection we did, variables we consider:\n",
    "\n",
    "1. sent_pos: Positive sentiment\n",
    "2. sent_neu: Neutral sentiment\n",
    "3. sent_neg: Negative sentiment\n",
    "4. discourse_comp: Discourse complexity (calculated through comparison, contingency, expansive, sum those values)\n",
    "5. plan: Planning level (calculated through future)\n",
    "6. argu_claim: Argumentation level calculated through claim\n",
    "7. argu_premise: Argumentation level calculated through premise\n",
    "8. n_request: Number of requests\n",
    "9. politeness: Politeness\n",
    "10. n_words: Number of words\n",
    "11. n_sentences: Number of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, input all the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "import math\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data file contains only the selected games, there is no need to consider the selection criteria when retreving the message data. \n",
    "*The condition \"relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time, with no more than five seasons passing between two acts of friendship\" is fullfilled.\n",
    "\n",
    "Here, we read the data from json file and get the number of games, which is 500, including 250 selected games ended up betrayal and 250 selected games ended up lasting friendship between two players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from diplomacy_data.json with reading mode\n",
    "with open(\"diplomacy_data.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only consider dialogs that happen before the last act of friendship, we need to know when does the last act of friendship happen. Later when extracting feature values, extract them from the seasons that is before the last support season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the season where the last support between two players happened\n",
    "# entry refer to one entry in the dataset, one game\n",
    "def last_support(entry):\n",
    "    seasons = entry['seasons']\n",
    "    last_support = None\n",
    "    for season in seasons[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the average value across the seasons for all features of the messages\n",
    "# msgs: messages sent by a player from a season in a game\n",
    "def extract_features(msgs):\n",
    "    n_sents = sum(m['n_sentences'] for m in msgs) * 1.0\n",
    "    if(n_sents==0.0):\n",
    "        print(msgs)\n",
    "    \n",
    "    # Sentiment\n",
    "    # compute positive sentiment score\n",
    "    sent_pos = sum(m['sentiment'].get(\"positive\") for m in msgs) / len(msgs)\n",
    "    # compute neutral sentiment score\n",
    "    sent_neu = sum(m['sentiment'].get(\"neutral\") for m in msgs) / len(msgs)\n",
    "    # compute positive sentiment score\n",
    "    sent_neg = sum(m['sentiment'].get(\"negative\") for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Argumentation and Discourse\n",
    "    # compute discourse complexity through comparison, contingency, expansive and temporal\n",
    "    comparison = sum(len(m['lexicon_words'].get(\"disc_comparison\", []))for m in msgs) / n_sents\n",
    "    contingency = sum(len(m['lexicon_words'].get(\"disc_contingency\", []))for m in msgs) / n_sents\n",
    "    expansive = sum(len(m['lexicon_words'].get(\"disc_expansion\", []))for m in msgs) / n_sents\n",
    "    discourse_comp = comparison+contingency+expansive\n",
    "    \n",
    "    # compute the average number of markers refer to future\n",
    "    plan = sum(len(m['lexicon_words'].get(\"disc_temporal_future\", []))for m in msgs) / n_sents\n",
    "    \n",
    "    # compute argumentation level through claim and premise\n",
    "    argu_claim = sum(len(m['lexicon_words'].get(\"claim\", []))for m in msgs) / n_sents\n",
    "    argu_premise = sum(len(m['lexicon_words'].get(\"premise\", []))for m in msgs) / n_sents\n",
    "    \n",
    "    # compute average number of request\n",
    "    n_request = sum(m['n_requests'] for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Politeness\n",
    "    politeness = sum(m['politeness'] for m in msgs) / len(msgs)\n",
    "    \n",
    "    # Talkativeness\n",
    "    # compute average number of words\n",
    "    n_words = sum(m['n_words'] for m in msgs) / len(msgs)\n",
    "    # compute average number of sentences\n",
    "    n_sentences = sum(m['n_sentences'] for m in msgs) / len(msgs)\n",
    "\n",
    "    return np.array([sent_pos, sent_neu, sent_neg, discourse_comp, plan, argu_claim, argu_premise, n_request, politeness, n_words, n_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From paper: To ensure that we are studying conversational patterns that occur only when the two individuals in the dyad are ostensibly being friends, we only extract features from the messages exchanged before the last act of friendship.\n",
    "Therefore, the messages we extract should from seasons less than last support season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get average politeness scores from dataset entries \n",
    "# data: the dataset where we extract politeness score from\n",
    "# betrayal: if the games we consider end up betrayal\n",
    "# betrayer: if the person to be analyzed is the (potential) betrayer\n",
    "def process_data(data, betrayal, betrayer):\n",
    "    results = []\n",
    "\n",
    "    # loop in every game in data\n",
    "    for entry in data:                        \n",
    "        if(entry['betrayal'] == betrayal):\n",
    "            for i, season in enumerate(entry['seasons']):\n",
    "                if(season['season'] <= last_support(entry)):\n",
    "                    if(len(season['messages']['betrayer']) > 0 and len(season['messages']['victim']) > 0):                \n",
    "                        if(betrayer):\n",
    "                            results.append(extract_features(season['messages']['betrayer']))\n",
    "                        else:\n",
    "                            results.append(extract_features(season['messages']['victim']))\n",
    "       \n",
    "\n",
    "    # return value is a list of features of the indicated player in the indicated type of games\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the politeness score of the four conditions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the games end up betrayal, get politeness score for each betrayer\n",
    "betray_er = pd.DataFrame(process_data(data, True, True))\n",
    "\n",
    "# for the games end up betrayal, get politeness score for each victim\n",
    "betray_vi = pd.DataFrame(process_data(data, True, False)) \n",
    "\n",
    "# for the games not end up betrayal, get politeness score for each potential betrayer\n",
    "control_er = pd.DataFrame(process_data(data, False, True))\n",
    "\n",
    "# for the games not end up betrayal, get politeness score for each potential victim\n",
    "control_vi = pd.DataFrame(process_data(data, False, False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_pos</th>\n",
       "      <th>sent_neu</th>\n",
       "      <th>sent_neg</th>\n",
       "      <th>discourse_comp</th>\n",
       "      <th>plan</th>\n",
       "      <th>argu_claim</th>\n",
       "      <th>argu_premise</th>\n",
       "      <th>n_request</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>betray</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.237419</td>\n",
       "      <td>-0.009032</td>\n",
       "      <td>-0.024516</td>\n",
       "      <td>-0.332903</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>-38.500000</td>\n",
       "      <td>-3.583333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.190476</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.076389</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>-0.225425</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>1.052198</td>\n",
       "      <td>0.271978</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>-0.060440</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.375372</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>-2.833333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.335738</td>\n",
       "      <td>-46.333333</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.419398</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.053745</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>-0.116667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.220882</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sent_pos  sent_neu  sent_neg  discourse_comp      plan  argu_claim  \\\n",
       "0    -0.166667  0.083333 -3.500000        0.237419 -0.009032   -0.024516   \n",
       "1    -1.190476  0.190476  0.285714        0.312500  0.125000    0.062500   \n",
       "2     1.000000 -1.166667 -2.666667        1.052198  0.271978    0.307692   \n",
       "3     0.800000  0.800000  0.200000        0.083333 -0.125000   -0.125000   \n",
       "4     0.333333 -0.666667 -1.666667        0.133333  0.033333   -0.066667   \n",
       "...        ...       ...       ...             ...       ...         ...   \n",
       "1377 -1.000000 -0.500000 -1.500000       -0.625000 -0.375000   -0.250000   \n",
       "1378  1.000000  5.000000  1.000000        0.250000  0.000000    0.125000   \n",
       "1379  2.000000  4.000000  6.000000        0.500000  0.250000    0.000000   \n",
       "1380 -2.000000  3.000000  5.000000        0.888889  0.333333    0.111111   \n",
       "1381  1.500000 -1.500000  3.500000        0.666667  0.133333   -0.116667   \n",
       "\n",
       "      argu_premise  n_request  politeness     n_words  n_sentences  betray  \n",
       "0        -0.332903   0.416667   -0.006665  -38.500000    -3.583333       1  \n",
       "1         0.076389   0.619048   -0.225425    9.000000    -0.714286       1  \n",
       "2        -0.060440   1.833333    0.375372   46.500000    -2.833333       1  \n",
       "3        -0.083333   2.700000    0.177667   61.800000     1.800000       1  \n",
       "4         0.133333  -0.333333    0.335738  -46.333333    -2.000000       1  \n",
       "...            ...        ...         ...         ...          ...     ...  \n",
       "1377     -0.250000   0.000000   -0.419398  -30.000000    -3.000000       0  \n",
       "1378      0.250000   2.000000    0.459459  101.000000     7.000000       0  \n",
       "1379      0.687500   6.000000    0.053745  249.000000    12.000000       0  \n",
       "1380      0.444444   6.000000    0.027826  195.000000     6.000000       0  \n",
       "1381      0.016667   3.000000    0.220882   64.000000     3.500000       0  \n",
       "\n",
       "[1382 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute imbalance score of both betrayal and control games, concat them to get a dataframe for betrayal prediction\n",
    "betray_imb = betray_er-betray_vi\n",
    "betray_imb['betray'] = 1\n",
    "control_imb = control_er-control_vi\n",
    "control_imb['betray'] = 0\n",
    "\n",
    "frames = [betray_imb, control_imb]\n",
    "Imbalance = pd.concat(frames,ignore_index=True)\n",
    "Imbalance = Imbalance.rename(columns={0: \"sent_pos\", 1: \"sent_neu\", 2: \"sent_neg\", 3: \"discourse_comp\", 4: \"plan\", 5: \"argu_claim\", 6: \"argu_premise\", 7: \"n_request\", 8: \"politeness\", 9: \"n_words\", 10:\"n_sentences\"})\n",
    "Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the dataset to train and test dataset at ratio 8:2\n",
    "train, test = train_test_split(Imbalance, test_size=0.2, random_state=0)\n",
    "trainX = train.drop(['betray'],axis=1)\n",
    "trainy = train['betray']\n",
    "testX = test.drop(['betray'],axis=1)\n",
    "testy = test['betray']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a logistic regression using the training dataset. We consider the p value of each variable to decide if the variable is important. However, we realize no matter how we try, the p value of some variables are still very high. The model we show here is the one with the highest predictive power, but we cannot really explain it since the p values are quite big. We therefore consider other models to do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.678994\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 betray   No. Observations:                 1105\n",
      "Model:                          Logit   Df Residuals:                     1092\n",
      "Method:                           MLE   Df Model:                           12\n",
      "Date:                Fri, 18 Dec 2020   Pseudo R-squ.:                 0.01732\n",
      "Time:                        20:58:02   Log-Likelihood:                -750.29\n",
      "converged:                       True   LL-Null:                       -763.51\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.009257\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  -0.1401      0.066     -2.122      0.034      -0.270      -0.011\n",
      "sent_pos                    0.1350      0.062      2.165      0.030       0.013       0.257\n",
      "sent_neu                   -0.0302      0.044     -0.686      0.492      -0.116       0.056\n",
      "sent_neg                    0.0448      0.053      0.840      0.401      -0.060       0.149\n",
      "discourse_comp             -0.3025      0.165     -1.835      0.066      -0.626       0.021\n",
      "plan                       -0.1608      0.478     -0.336      0.737      -1.098       0.776\n",
      "discourse_comp:plan        -0.4063      0.239     -1.700      0.089      -0.875       0.062\n",
      "argu_claim:argu_premise    -2.0495      1.351     -1.517      0.129      -4.697       0.598\n",
      "argu_premise               -0.1464      0.239     -0.612      0.541      -0.615       0.323\n",
      "n_request                   0.0543      0.046      1.172      0.241      -0.036       0.145\n",
      "politeness                  0.2077      0.290      0.716      0.474      -0.361       0.776\n",
      "n_words                    -0.0018      0.002     -0.784      0.433      -0.006       0.003\n",
      "n_words:n_sentences      8.099e-05   6.19e-05      1.309      0.191   -4.03e-05       0.000\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression to predict betrayal \n",
    "mod = smf.logit(formula='betray ~ sent_pos+sent_neu+sent_neg+discourse_comp*plan+argu_claim:argu_premise+argu_premise+n_request+politeness+n_words+n_words:n_sentences', data=train)\n",
    "res = mod.fit(maxiter=30)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the predictive power of the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[58, 72],\n",
       "       [48, 99]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict for test set\n",
    "prob = res.predict(exog = testX)\n",
    "pred = np.where(prob >= 0.45, 1, 0)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm = metrics.confusion_matrix(testy, pred, labels=[0,1])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5667870036101083"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the accuracy of the model\n",
    "accuracy = cm.diagonal().sum()/cm.sum()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try SVC and random forest to do the classification,the results are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5303876942395228"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try SVC\n",
    "sv =SVC(C=10, kernel='rbf', gamma='auto', coef0=0.0, shrinking=True, probability=False,\n",
    "        decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "\n",
    "sv_results = cross_validate(sv, Imbalance.drop(['betray'],axis=1), Imbalance['betray'], cv=5)\n",
    "np.mean(sv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48193114634018724"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try random forest\n",
    "rf= RandomForestClassifier(n_estimators=300,max_depth=22,criterion='gini')\n",
    "\n",
    "rf_results = cross_validate(rf, Imbalance.drop(['betray'],axis=1), Imbalance['betray'], cv=5)\n",
    "np.mean(rf_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not so satisfied with the predictive power, therefore, we think about predict betrayal with four seasons in a time line, that leads us to the second part of our experiment in part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
